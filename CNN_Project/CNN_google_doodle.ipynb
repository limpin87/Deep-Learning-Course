{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sutils import *\n",
    "import os, json\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten,Input, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import multi_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n",
      "2.0.8\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  345\n"
     ]
    }
   ],
   "source": [
    "fnames =  glob('/../../nfs/p4/shared/datasets/quick_draw/numpy_bitmap/*')\n",
    "# for entry in fnames:\n",
    "#     print entry\n",
    "print 'Number of classes: ', len(fnames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n"
     ]
    }
   ],
   "source": [
    "## Creating the training data set\n",
    "Npc = 8000 # Number of images to use for each class \n",
    "Npc_t = 2000  # Number of images to use for testing for each class \n",
    "n_class = 345 # Number of classes to use \n",
    "\n",
    "for i, entry in enumerate(fnames):\n",
    "    print i\n",
    "    if i >= n_class:\n",
    "        break\n",
    "    data = np.load(entry)[:Npc]\n",
    "    labels =  np.zeros([data.shape[0],n_class])\n",
    "    labels[:,i] = 1\n",
    "    if i == 0:\n",
    "        X = data[:Npc]\n",
    "        targets = labels\n",
    "    else:\n",
    "        X = np.vstack([X, data])\n",
    "        targets = np.vstack([targets,labels])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2760000, 784)\n",
      "Target shape: (2760000, 345)\n"
     ]
    }
   ],
   "source": [
    "print 'Input shape:', X.shape\n",
    "print 'Target shape:', targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Inp = Input(shape=(28,28,1),name = 'Input_layer')\n",
    "\n",
    "#ConvBlock 01\n",
    "conv01 = Conv2D(64, (5, 5), padding='same',activation = 'relu', input_shape=Inp.shape,name = 'Conv01_layer')(Inp)\n",
    "conv02 = Conv2D(64, (5, 5),activation = 'relu',name = 'Conv02_layer')(conv01)\n",
    "maxpool_01 = MaxPooling2D(pool_size=(2, 2),name = 'MaxPool01_layer')(conv02)\n",
    "drop01 = Dropout(0.25,name = 'Dropout01_layer')(maxpool_01)\n",
    "\n",
    "#Convblock 02\n",
    "conv03 = Conv2D(128, (3, 3), padding='same',activation = 'relu',name = 'Conv03_layer')(drop01)\n",
    "conv04 = Conv2D(128, (3, 3),activation = 'relu',name = 'Conv04_layer')(conv03)\n",
    "maxpool_02 = MaxPooling2D(pool_size=(2, 2),name = 'MaxPool02_layer')(conv04)\n",
    "drop02 = Dropout(0.25,name = 'Dropout02_layer')(maxpool_02)\n",
    "\n",
    "# Fully Connected Dense block\n",
    "x = Flatten(name = 'Flatten_layer')(drop02)\n",
    "x = Dense(512, activation='relu',name = 'Dense01_layer')(x)\n",
    "x = Dropout(0.5,name = 'Dropout03_layer')(x)\n",
    "logits_layer = Dense(y_train.shape[1], name= 'logits_layer')(x)\n",
    "output = Activation('softmax',name = 'Sofftmax_layer')(logits_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model inputs and output\n",
    "model = Model(Inp, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_layer (InputLayer)     (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv01_layer (Conv2D)        (None, 28, 28, 64)        1664      \n",
      "_________________________________________________________________\n",
      "Conv02_layer (Conv2D)        (None, 24, 24, 64)        102464    \n",
      "_________________________________________________________________\n",
      "MaxPool01_layer (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "Dropout01_layer (Dropout)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv03_layer (Conv2D)        (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "Conv04_layer (Conv2D)        (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "MaxPool02_layer (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "Dropout02_layer (Dropout)    (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "Flatten_layer (Flatten)      (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "Dense01_layer (Dense)        (None, 512)               1638912   \n",
      "_________________________________________________________________\n",
      "Dropout03_layer (Dropout)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "logits_layer (Dense)         (None, 345)               176985    \n",
      "_________________________________________________________________\n",
      "Sofftmax_layer (Activation)  (None, 345)               0         \n",
      "=================================================================\n",
      "Total params: 2,141,465\n",
      "Trainable params: 2,141,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi_gpu.py:44: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
      "/nfs/p4/ceusers/limpin/.venv2.7-gpu/local/lib/python2.7/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "multi_gpu.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "  return Model(input=model.inputs, output=merged)\n"
     ]
    }
   ],
   "source": [
    "model = multi_gpu.make_parallel(model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "# num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_simple_CNN.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14446/14446 [==============================] - 866s - loss: 3.7796 - acc: 0.2168   -\n",
      "Epoch 2/100\n",
      "14446/14446 [==============================] - 799s - loss: 2.8481 - acc: 0.3711   \n",
      "Epoch 3/100\n",
      "14446/14446 [==============================] - 798s - loss: 2.6479 - acc: 0.4113   \n",
      "Epoch 4/100\n",
      "14446/14446 [==============================] - 809s - loss: 2.5704 - acc: 0.4282   \n",
      "Epoch 5/100\n",
      "14446/14446 [==============================] - 797s - loss: 2.5338 - acc: 0.4365   \n",
      "Epoch 6/100\n",
      "14446/14446 [==============================] - 794s - loss: 2.5109 - acc: 0.4415   \n",
      "Epoch 7/100\n",
      "14446/14446 [==============================] - 777s - loss: 2.5012 - acc: 0.4431   \n",
      "Epoch 8/100\n",
      "14446/14446 [==============================] - 811s - loss: 2.4918 - acc: 0.4459   \n",
      "Epoch 9/100\n",
      "14446/14446 [==============================] - 810s - loss: 2.4847 - acc: 0.4476   \n",
      "Epoch 10/100\n",
      "14446/14446 [==============================] - 805s - loss: 2.4808 - acc: 0.4485   \n",
      "Epoch 11/100\n",
      "14446/14446 [==============================] - 817s - loss: 2.4770 - acc: 0.4498   \n",
      "Epoch 12/100\n",
      "14446/14446 [==============================] - 810s - loss: 2.4766 - acc: 0.4498   \n",
      "Epoch 13/100\n",
      "14446/14446 [==============================] - 809s - loss: 2.4771 - acc: 0.4500   \n",
      "Epoch 14/100\n",
      "14446/14446 [==============================] - 812s - loss: 2.4799 - acc: 0.4498   \n",
      "Epoch 15/100\n",
      "14446/14446 [==============================] - 812s - loss: 2.4870 - acc: 0.4492   \n",
      "Epoch 16/100\n",
      "14446/14446 [==============================] - 791s - loss: 2.4904 - acc: 0.4489   \n",
      "Epoch 17/100\n",
      "14446/14446 [==============================] - 802s - loss: 2.4979 - acc: 0.4478   - ETA: 4s - loss: 2.4980 - acc: 0.\n",
      "Epoch 18/100\n",
      "14446/14446 [==============================] - 799s - loss: 2.5042 - acc: 0.4471   \n",
      "Epoch 19/100\n",
      "14446/14446 [==============================] - 785s - loss: 2.5138 - acc: 0.4463   \n",
      "Epoch 20/100\n",
      "14446/14446 [==============================] - 786s - loss: 2.5219 - acc: 0.4448   \n",
      "Epoch 21/100\n",
      "14446/14446 [==============================] - 799s - loss: 2.5293 - acc: 0.4438   \n",
      "Epoch 22/100\n",
      "14446/14446 [==============================] - 807s - loss: 2.5416 - acc: 0.4421   \n",
      "Epoch 23/100\n",
      "14446/14446 [==============================] - 810s - loss: 2.5480 - acc: 0.4411   \n",
      "Epoch 24/100\n",
      "14446/14446 [==============================] - 813s - loss: 2.5593 - acc: 0.4391   - ETA: 2s - loss:\n",
      "Epoch 25/100\n",
      "14446/14446 [==============================] - 766s - loss: 2.5695 - acc: 0.4379   \n",
      "Epoch 26/100\n",
      "14446/14446 [==============================] - 803s - loss: 2.5803 - acc: 0.4363   \n",
      "Epoch 27/100\n",
      "14446/14446 [==============================] - 799s - loss: 2.5911 - acc: 0.4347   \n",
      "Epoch 28/100\n",
      "14446/14446 [==============================] - 607s - loss: 2.6013 - acc: 0.4329   \n",
      "Epoch 29/100\n",
      "14446/14446 [==============================] - 723s - loss: 2.6145 - acc: 0.4304   \n",
      "Epoch 30/100\n",
      "14446/14446 [==============================] - 793s - loss: 2.6257 - acc: 0.4284   \n",
      "Epoch 31/100\n",
      "14446/14446 [==============================] - 797s - loss: 2.6371 - acc: 0.4266   - ETA: 2s -\n",
      "Epoch 32/100\n",
      "14446/14446 [==============================] - 797s - loss: 2.6471 - acc: 0.4247   - ETA: 0s - loss: 2.6471 - acc: \n",
      "Epoch 33/100\n",
      "14446/14446 [==============================] - 801s - loss: 2.6582 - acc: 0.4232   \n",
      "Epoch 34/100\n",
      "14446/14446 [==============================] - 801s - loss: 2.6655 - acc: 0.4220   \n",
      "Epoch 35/100\n",
      "14446/14446 [==============================] - 790s - loss: 2.6762 - acc: 0.4202   \n",
      "Epoch 36/100\n",
      "14446/14446 [==============================] - 784s - loss: 2.6871 - acc: 0.4185   \n",
      "Epoch 37/100\n",
      "14446/14446 [==============================] - 799s - loss: 2.7000 - acc: 0.4156   \n",
      "Epoch 38/100\n",
      "14446/14446 [==============================] - 801s - loss: 2.7088 - acc: 0.4144   \n",
      "Epoch 39/100\n",
      "14446/14446 [==============================] - 608s - loss: 2.7194 - acc: 0.4128   \n",
      "Epoch 40/100\n",
      "14446/14446 [==============================] - 612s - loss: 2.7308 - acc: 0.4106   \n",
      "Epoch 41/100\n",
      "14446/14446 [==============================] - 612s - loss: 2.7408 - acc: 0.4087   - ETA: 0s - loss: 2.7407 - a\n",
      "Epoch 42/100\n",
      "14446/14446 [==============================] - 608s - loss: 2.7529 - acc: 0.4069   \n",
      "Epoch 43/100\n",
      "14446/14446 [==============================] - 614s - loss: 2.7620 - acc: 0.4052   \n",
      "Epoch 44/100\n",
      "14446/14446 [==============================] - 797s - loss: 2.7714 - acc: 0.4037   \n",
      "Epoch 45/100\n",
      "14446/14446 [==============================] - 764s - loss: 2.7835 - acc: 0.4010   \n",
      "Epoch 46/100\n",
      "14446/14446 [==============================] - 803s - loss: 2.7930 - acc: 0.4001   \n",
      "Epoch 47/100\n",
      "14446/14446 [==============================] - 799s - loss: 2.7999 - acc: 0.3983   \n",
      "Epoch 48/100\n",
      "14446/14446 [==============================] - 800s - loss: 2.8093 - acc: 0.3964   \n",
      "Epoch 49/100\n",
      "14446/14446 [==============================] - 610s - loss: 2.8182 - acc: 0.3954   \n",
      "Epoch 50/100\n",
      "14446/14446 [==============================] - 608s - loss: 2.8287 - acc: 0.3933   \n",
      "Epoch 51/100\n",
      "14446/14446 [==============================] - 609s - loss: 2.8380 - acc: 0.3911   \n",
      "Epoch 52/100\n",
      "14446/14446 [==============================] - 610s - loss: 2.8452 - acc: 0.3898   \n",
      "Epoch 53/100\n",
      "14446/14446 [==============================] - 800s - loss: 2.8588 - acc: 0.3880   \n",
      "Epoch 54/100\n",
      "14446/14446 [==============================] - 801s - loss: 2.8657 - acc: 0.3866   \n",
      "Epoch 55/100\n",
      "14446/14446 [==============================] - 776s - loss: 2.8728 - acc: 0.3857   \n",
      "Epoch 56/100\n",
      "14446/14446 [==============================] - 758s - loss: 2.8838 - acc: 0.3833   \n",
      "Epoch 57/100\n",
      "14446/14446 [==============================] - 803s - loss: 2.8943 - acc: 0.3818   \n",
      "Epoch 58/100\n",
      "14446/14446 [==============================] - 798s - loss: 2.9045 - acc: 0.3805   \n",
      "Epoch 59/100\n",
      "14446/14446 [==============================] - 800s - loss: 2.9131 - acc: 0.3787   \n",
      "Epoch 60/100\n",
      "14446/14446 [==============================] - 801s - loss: 2.9227 - acc: 0.3768   - ETA: 0s - loss: 2.9227 - acc: 0.3\n",
      "Epoch 61/100\n",
      "14446/14446 [==============================] - 806s - loss: 2.9361 - acc: 0.3749   \n",
      "Epoch 62/100\n",
      "14446/14446 [==============================] - 795s - loss: 2.9435 - acc: 0.3724   \n",
      "Epoch 63/100\n",
      "14446/14446 [==============================] - 790s - loss: 2.9555 - acc: 0.3710   \n",
      "Epoch 64/100\n",
      "14446/14446 [==============================] - 795s - loss: 2.9645 - acc: 0.3688   \n",
      "Epoch 65/100\n",
      "14446/14446 [==============================] - 729s - loss: 2.9784 - acc: 0.3666   \n",
      "Epoch 66/100\n",
      "14446/14446 [==============================] - 755s - loss: 2.9862 - acc: 0.3654   \n",
      "Epoch 67/100\n",
      "14446/14446 [==============================] - 793s - loss: 2.9970 - acc: 0.3635   \n",
      "Epoch 68/100\n",
      "14446/14446 [==============================] - 800s - loss: 3.0057 - acc: 0.3623   \n",
      "Epoch 69/100\n",
      "14446/14446 [==============================] - 794s - loss: 3.0186 - acc: 0.3599   \n",
      "Epoch 70/100\n",
      "14446/14446 [==============================] - 774s - loss: 3.0296 - acc: 0.3580   \n",
      "Epoch 71/100\n",
      "14446/14446 [==============================] - 789s - loss: 3.0405 - acc: 0.3561   \n",
      "Epoch 72/100\n",
      "14446/14446 [==============================] - 794s - loss: 3.0540 - acc: 0.3541   \n",
      "Epoch 73/100\n",
      "14446/14446 [==============================] - 604s - loss: 3.0656 - acc: 0.3518   \n",
      "Epoch 74/100\n",
      "14446/14446 [==============================] - 594s - loss: 3.0770 - acc: 0.3501   \n",
      "Epoch 75/100\n",
      "14446/14446 [==============================] - 601s - loss: 3.0868 - acc: 0.3477   - ETA: 1s - loss: 3\n",
      "Epoch 76/100\n",
      "14446/14446 [==============================] - 506s - loss: 3.0990 - acc: 0.3456   \n",
      "Epoch 77/100\n",
      "14446/14446 [==============================] - 760s - loss: 3.1100 - acc: 0.3438   \n",
      "Epoch 78/100\n",
      "14446/14446 [==============================] - 795s - loss: 3.1233 - acc: 0.3414   \n",
      "Epoch 79/100\n",
      "14446/14446 [==============================] - 790s - loss: 3.1376 - acc: 0.3391   -  - ETA: 2s - \n",
      "Epoch 80/100\n",
      "14446/14446 [==============================] - 801s - loss: 3.1474 - acc: 0.3369   \n",
      "Epoch 81/100\n",
      "14446/14446 [==============================] - 788s - loss: 3.1581 - acc: 0.3350   \n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14446/14446 [==============================] - 794s - loss: 3.1750 - acc: 0.3320   \n",
      "Epoch 83/100\n",
      "14446/14446 [==============================] - 798s - loss: 3.1853 - acc: 0.3294   \n",
      "Epoch 84/100\n",
      "14446/14446 [==============================] - 800s - loss: 3.1967 - acc: 0.3273   \n",
      "Epoch 85/100\n",
      "14446/14446 [==============================] - 800s - loss: 3.2097 - acc: 0.3255   \n",
      "Epoch 86/100\n",
      "14446/14446 [==============================] - 730s - loss: 3.2183 - acc: 0.3230   \n",
      "Epoch 87/100\n",
      "14446/14446 [==============================] - 763s - loss: 3.2270 - acc: 0.3215   \n",
      "Epoch 88/100\n",
      "14446/14446 [==============================] - 794s - loss: 3.2401 - acc: 0.3193   \n",
      "Epoch 89/100\n",
      "14446/14446 [==============================] - 611s - loss: 3.2506 - acc: 0.3168   \n",
      "Epoch 90/100\n",
      "14446/14446 [==============================] - 611s - loss: 3.2577 - acc: 0.3152   \n",
      "Epoch 91/100\n",
      "14446/14446 [==============================] - 788s - loss: 3.2665 - acc: 0.3138   \n",
      "Epoch 92/100\n",
      "14446/14446 [==============================] - 787s - loss: 3.2723 - acc: 0.3121   \n",
      "Epoch 93/100\n",
      "14446/14446 [==============================] - 792s - loss: 3.2808 - acc: 0.3102   \n",
      "Epoch 94/100\n",
      "14446/14446 [==============================] - 793s - loss: 3.2865 - acc: 0.3093   - ETA: 0s - loss: 3.2865 - acc: 0.3\n",
      "Epoch 95/100\n",
      "14446/14446 [==============================] - 790s - loss: 3.2978 - acc: 0.3073   \n",
      "Epoch 96/100\n",
      "14446/14446 [==============================] - 779s - loss: 3.3049 - acc: 0.3061   \n",
      "Epoch 97/100\n",
      "14446/14446 [==============================] - 779s - loss: 3.3187 - acc: 0.3028   \n",
      "Epoch 98/100\n",
      "14446/14446 [==============================] - 824s - loss: 3.3223 - acc: 0.3017   \n",
      "Epoch 99/100\n",
      "14446/14446 [==============================] - 804s - loss: 3.3242 - acc: 0.3013   \n",
      "Epoch 100/100\n",
      "14446/14446 [==============================] - 808s - loss: 3.3287 - acc: 0.3005   \n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                            steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                            epochs=epochs,\n",
    "#                             validation_data=(X_test, y_test),\n",
    "                            workers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_train(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
