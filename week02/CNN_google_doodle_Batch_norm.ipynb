{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sutils import *\n",
    "import os, json\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten,Input, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping,LearningRateScheduler,ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "import multi_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n",
      "2.0.8\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  345\n"
     ]
    }
   ],
   "source": [
    "fnames =  glob('/../../nfs/p4/shared/datasets/quick_draw/numpy_bitmap/*')\n",
    "# for entry in fnames:\n",
    "#     print entry\n",
    "print 'Number of classes: ', len(fnames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "## Creating the training data set\n",
    "Npc = 8000 # Number of images to use for each class \n",
    "Npc_t = 2000  # Number of images to use for testing for each class \n",
    "n_class = 100 # Number of classes to use \n",
    "\n",
    "for i, entry in enumerate(fnames):\n",
    "    print i\n",
    "    if i >= n_class:\n",
    "        break\n",
    "    data = np.load(entry)[:Npc]\n",
    "    labels =  np.zeros([data.shape[0],n_class])\n",
    "    labels[:,i] = 1\n",
    "    if i == 0:\n",
    "        X = data[:Npc]\n",
    "        targets = labels\n",
    "    else:\n",
    "        X = np.vstack([X, data])\n",
    "        targets = np.vstack([targets,labels])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (800000, 784)\n",
      "Target shape: (800000, 100)\n"
     ]
    }
   ],
   "source": [
    "print 'Input shape:', X.shape\n",
    "print 'Target shape:', targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Inp = Input(shape=(28,28,1),name = 'Input_layer')\n",
    "\n",
    "#ConvBlock 01\n",
    "conv01 = Conv2D(64, (5, 5), padding='same',activation = 'relu', input_shape=Inp.shape,name = 'Conv01_layer')(Inp)\n",
    "conv01 = BatchNormalization()(conv01)\n",
    "conv02 = Conv2D(64, (5, 5),activation = 'relu',name = 'Conv02_layer')(conv01)\n",
    "maxpool_01 = MaxPooling2D(pool_size=(2, 2),name = 'MaxPool01_layer')(conv02)\n",
    "drop01 = Dropout(0.25,name = 'Dropout01_layer')(maxpool_01)\n",
    "\n",
    "#Convblock 02\n",
    "conv03 = Conv2D(128, (3, 3), padding='same',activation = 'relu',name = 'Conv03_layer')(drop01)\n",
    "conv03 = BatchNormalization()(conv03)\n",
    "conv04 = Conv2D(128, (3, 3),activation = 'relu',name = 'Conv04_layer')(conv03)\n",
    "maxpool_02 = MaxPooling2D(pool_size=(2, 2),name = 'MaxPool02_layer')(conv04)\n",
    "drop02 = Dropout(0.25,name = 'Dropout02_layer')(maxpool_02)\n",
    "\n",
    "# Fully Connected Dense block\n",
    "x = Flatten(name = 'Flatten_layer')(drop02)\n",
    "x = Dense(512, activation='relu',name = 'Dense01_layer')(x)\n",
    "x = Dropout(0.5,name = 'Dropout03_layer')(x)\n",
    "x = BatchNormalization()(x)\n",
    "logits_layer = Dense(y_train.shape[1], name= 'logits_layer')(x)\n",
    "output = Activation('softmax',name = 'Sofftmax_layer')(logits_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model inputs and output\n",
    "model = Model(Inp, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_layer (InputLayer)     (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv01_layer (Conv2D)        (None, 28, 28, 64)        1664      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "Conv02_layer (Conv2D)        (None, 24, 24, 64)        102464    \n",
      "_________________________________________________________________\n",
      "MaxPool01_layer (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "Dropout01_layer (Dropout)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv03_layer (Conv2D)        (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "Conv04_layer (Conv2D)        (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "MaxPool02_layer (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "Dropout02_layer (Dropout)    (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "Flatten_layer (Flatten)      (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "Dense01_layer (Dense)        (None, 512)               1638912   \n",
      "_________________________________________________________________\n",
      "Dropout03_layer (Dropout)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "logits_layer (Dense)         (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "Sofftmax_layer (Activation)  (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 2,018,596\n",
      "Trainable params: 2,017,188\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multi_gpu.py:44: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
      "/nfs/p4/ceusers/limpin/.venv2.7-gpu/local/lib/python2.7/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "multi_gpu.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n",
      "  return Model(input=model.inputs, output=merged)\n"
     ]
    }
   ],
   "source": [
    "model = multi_gpu.make_parallel(model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "callbacks = [ReduceLROnPlateau(monitor='loss'),\n",
    "            TensorBoard(log_dir=\"logs/{0}\".format(\"quickdraw_lr0.0001_dc1e-6\"))]\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "# num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_simple_CNN.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4187/4187 [==============================] - 297s - loss: 3.2416 - acc: 0.2404 - val_loss: 1.8676 - val_acc: 0.5290\n",
      "Epoch 2/100\n",
      "4187/4187 [==============================] - 283s - loss: 2.2509 - acc: 0.4347 - val_loss: 1.4843 - val_acc: 0.6181\n",
      "Epoch 3/100\n",
      "4187/4187 [==============================] - 281s - loss: 1.9302 - acc: 0.5073 - val_loss: 1.2947 - val_acc: 0.6628\n",
      "Epoch 4/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.7688 - acc: 0.5455 - val_loss: 1.2136 - val_acc: 0.6836\n",
      "Epoch 5/100\n",
      "4187/4187 [==============================] - 282s - loss: 1.6710 - acc: 0.5691 - val_loss: 1.1458 - val_acc: 0.7016\n",
      "Epoch 6/100\n",
      "4187/4187 [==============================] - 280s - loss: 1.6037 - acc: 0.5864 - val_loss: 1.1067 - val_acc: 0.7111\n",
      "Epoch 7/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.5534 - acc: 0.5990 - val_loss: 1.0852 - val_acc: 0.7160\n",
      "Epoch 8/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.5155 - acc: 0.6089 - val_loss: 1.0535 - val_acc: 0.7253\n",
      "Epoch 9/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.4837 - acc: 0.6173 - val_loss: 1.0317 - val_acc: 0.7304\n",
      "Epoch 10/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.4583 - acc: 0.6234 - val_loss: 1.0266 - val_acc: 0.7318\n",
      "Epoch 11/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.4370 - acc: 0.6294 - val_loss: 0.9994 - val_acc: 0.7386\n",
      "Epoch 12/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.4192 - acc: 0.6340 - val_loss: 0.9917 - val_acc: 0.7420\n",
      "Epoch 13/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.4026 - acc: 0.6378 - val_loss: 1.0030 - val_acc: 0.7396\n",
      "Epoch 14/100\n",
      "4187/4187 [==============================] - 273s - loss: 1.3847 - acc: 0.6420 - val_loss: 0.9665 - val_acc: 0.7476\n",
      "Epoch 15/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.3726 - acc: 0.6454 - val_loss: 0.9571 - val_acc: 0.7494\n",
      "Epoch 16/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.3630 - acc: 0.6478 - val_loss: 0.9672 - val_acc: 0.7488\n",
      "Epoch 17/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.3534 - acc: 0.6506 - val_loss: 0.9547 - val_acc: 0.7514\n",
      "Epoch 18/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.3485 - acc: 0.6522 - val_loss: 0.9527 - val_acc: 0.7506\n",
      "Epoch 19/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.3391 - acc: 0.6551 - val_loss: 0.9625 - val_acc: 0.7509\n",
      "Epoch 20/100\n",
      "4187/4187 [==============================] - 280s - loss: 1.3311 - acc: 0.6565 - val_loss: 0.9403 - val_acc: 0.7550\n",
      "Epoch 21/100\n",
      "4187/4187 [==============================] - 274s - loss: 1.3267 - acc: 0.6579 - val_loss: 0.9398 - val_acc: 0.7556\n",
      "Epoch 22/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.3217 - acc: 0.6599 - val_loss: 0.9437 - val_acc: 0.7545\n",
      "Epoch 23/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.3137 - acc: 0.6616 - val_loss: 0.9367 - val_acc: 0.7575\n",
      "Epoch 24/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.3110 - acc: 0.6626 - val_loss: 0.9237 - val_acc: 0.7593\n",
      "Epoch 25/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.3073 - acc: 0.6635 - val_loss: 0.9228 - val_acc: 0.7599\n",
      "Epoch 26/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.3025 - acc: 0.6650 - val_loss: 0.9189 - val_acc: 0.7609\n",
      "Epoch 27/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2994 - acc: 0.6661 - val_loss: 0.9166 - val_acc: 0.7622\n",
      "Epoch 28/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2972 - acc: 0.6668 - val_loss: 0.9248 - val_acc: 0.7606\n",
      "Epoch 29/100\n",
      "4187/4187 [==============================] - 273s - loss: 1.2932 - acc: 0.6683 - val_loss: 0.9292 - val_acc: 0.7605\n",
      "Epoch 30/100\n",
      "4187/4187 [==============================] - 272s - loss: 1.2894 - acc: 0.6685 - val_loss: 0.9142 - val_acc: 0.7626\n",
      "Epoch 31/100\n",
      "4187/4187 [==============================] - 271s - loss: 1.2864 - acc: 0.6706 - val_loss: 0.9157 - val_acc: 0.7623\n",
      "Epoch 32/100\n",
      "4187/4187 [==============================] - 273s - loss: 1.2855 - acc: 0.6701 - val_loss: 0.9069 - val_acc: 0.7648\n",
      "Epoch 33/100\n",
      "4187/4187 [==============================] - 272s - loss: 1.2823 - acc: 0.6706 - val_loss: 0.9096 - val_acc: 0.7648\n",
      "Epoch 34/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2788 - acc: 0.6723 - val_loss: 0.9089 - val_acc: 0.7639\n",
      "Epoch 35/100\n",
      "4187/4187 [==============================] - 273s - loss: 1.2793 - acc: 0.6720 - val_loss: 0.9065 - val_acc: 0.7652\n",
      "Epoch 36/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2754 - acc: 0.6730 - val_loss: 0.9033 - val_acc: 0.7651\n",
      "Epoch 37/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.2735 - acc: 0.6741 - val_loss: 0.9003 - val_acc: 0.7666\n",
      "Epoch 38/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2711 - acc: 0.6744 - val_loss: 0.9017 - val_acc: 0.7670\n",
      "Epoch 39/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2687 - acc: 0.6758 - val_loss: 0.8967 - val_acc: 0.7679\n",
      "Epoch 40/100\n",
      "4187/4187 [==============================] - 274s - loss: 1.2707 - acc: 0.6739 - val_loss: 0.9000 - val_acc: 0.7672\n",
      "Epoch 41/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2648 - acc: 0.6762 - val_loss: 0.9010 - val_acc: 0.7667\n",
      "Epoch 42/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2661 - acc: 0.6754 - val_loss: 0.8918 - val_acc: 0.7681\n",
      "Epoch 43/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2621 - acc: 0.6772 - val_loss: 0.9000 - val_acc: 0.7678\n",
      "Epoch 44/100\n",
      "4187/4187 [==============================] - 274s - loss: 1.2630 - acc: 0.6766 - val_loss: 0.9015 - val_acc: 0.7668\n",
      "Epoch 45/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2586 - acc: 0.6773 - val_loss: 0.9016 - val_acc: 0.7677\n",
      "Epoch 46/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2601 - acc: 0.6778 - val_loss: 0.8883 - val_acc: 0.7695\n",
      "Epoch 47/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2574 - acc: 0.6785 - val_loss: 0.8985 - val_acc: 0.7682\n",
      "Epoch 48/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2568 - acc: 0.6784 - val_loss: 0.9041 - val_acc: 0.7678\n",
      "Epoch 49/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2556 - acc: 0.6787 - val_loss: 0.8885 - val_acc: 0.7708\n",
      "Epoch 50/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2532 - acc: 0.6799 - val_loss: 0.8890 - val_acc: 0.7701\n",
      "Epoch 51/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.2535 - acc: 0.6801 - val_loss: 0.8889 - val_acc: 0.7702\n",
      "Epoch 52/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2525 - acc: 0.6801 - val_loss: 0.8876 - val_acc: 0.7705\n",
      "Epoch 53/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2518 - acc: 0.6801 - val_loss: 0.8874 - val_acc: 0.7706\n",
      "Epoch 54/100\n",
      "4187/4187 [==============================] - 273s - loss: 1.2502 - acc: 0.6803 - val_loss: 0.9098 - val_acc: 0.7662\n",
      "Epoch 55/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2469 - acc: 0.6811 - val_loss: 0.8919 - val_acc: 0.7695\n",
      "Epoch 56/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.2487 - acc: 0.6812 - val_loss: 0.8893 - val_acc: 0.7709\n",
      "Epoch 57/100\n",
      "4187/4187 [==============================] - 273s - loss: 1.2479 - acc: 0.6816 - val_loss: 0.9001 - val_acc: 0.7676\n",
      "Epoch 58/100\n",
      "4187/4187 [==============================] - 274s - loss: 1.2456 - acc: 0.6821 - val_loss: 0.8831 - val_acc: 0.7710\n",
      "Epoch 59/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2450 - acc: 0.6820 - val_loss: 0.8915 - val_acc: 0.7713\n",
      "Epoch 60/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.2448 - acc: 0.6821 - val_loss: 0.8789 - val_acc: 0.7727\n",
      "Epoch 61/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2435 - acc: 0.6830 - val_loss: 0.8882 - val_acc: 0.7711\n",
      "Epoch 62/100\n",
      "4187/4187 [==============================] - 274s - loss: 1.2444 - acc: 0.6821 - val_loss: 0.8760 - val_acc: 0.7736\n",
      "Epoch 63/100\n",
      "4187/4187 [==============================] - 275s - loss: 1.2417 - acc: 0.6830 - val_loss: 0.8870 - val_acc: 0.7728\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4187/4187 [==============================] - 278s - loss: 1.2419 - acc: 0.6829 - val_loss: 0.8814 - val_acc: 0.7718\n",
      "Epoch 65/100\n",
      "4187/4187 [==============================] - 280s - loss: 1.2403 - acc: 0.6839 - val_loss: 0.8750 - val_acc: 0.7734\n",
      "Epoch 66/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2396 - acc: 0.6834 - val_loss: 0.8829 - val_acc: 0.7718\n",
      "Epoch 67/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2366 - acc: 0.6847 - val_loss: 0.8806 - val_acc: 0.7725\n",
      "Epoch 68/100\n",
      "4187/4187 [==============================] - 280s - loss: 1.2368 - acc: 0.6844 - val_loss: 0.8771 - val_acc: 0.7731\n",
      "Epoch 69/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2384 - acc: 0.6843 - val_loss: 0.8830 - val_acc: 0.7723\n",
      "Epoch 70/100\n",
      "4187/4187 [==============================] - 280s - loss: 1.2382 - acc: 0.6844 - val_loss: 0.8806 - val_acc: 0.7731\n",
      "Epoch 71/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2353 - acc: 0.6847 - val_loss: 0.8768 - val_acc: 0.7738\n",
      "Epoch 72/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2339 - acc: 0.6849 - val_loss: 0.8780 - val_acc: 0.7727\n",
      "Epoch 73/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2344 - acc: 0.6859 - val_loss: 0.8724 - val_acc: 0.7750\n",
      "Epoch 74/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2337 - acc: 0.6847 - val_loss: 0.8724 - val_acc: 0.7741\n",
      "Epoch 75/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2343 - acc: 0.6854 - val_loss: 0.8761 - val_acc: 0.7736\n",
      "Epoch 76/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2311 - acc: 0.6854 - val_loss: 0.8735 - val_acc: 0.7750\n",
      "Epoch 77/100\n",
      "4187/4187 [==============================] - 281s - loss: 1.2297 - acc: 0.6867 - val_loss: 0.8772 - val_acc: 0.7742\n",
      "Epoch 78/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2303 - acc: 0.6860 - val_loss: 0.8727 - val_acc: 0.7746\n",
      "Epoch 79/100\n",
      "4187/4187 [==============================] - 280s - loss: 1.2294 - acc: 0.6857 - val_loss: 0.8764 - val_acc: 0.7750\n",
      "Epoch 80/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2293 - acc: 0.6866 - val_loss: 0.8718 - val_acc: 0.7752\n",
      "Epoch 81/100\n",
      "4187/4187 [==============================] - 280s - loss: 1.2305 - acc: 0.6865 - val_loss: 0.8782 - val_acc: 0.7734\n",
      "Epoch 82/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2285 - acc: 0.6859 - val_loss: 0.8702 - val_acc: 0.7759\n",
      "Epoch 83/100\n",
      "4187/4187 [==============================] - 281s - loss: 1.2285 - acc: 0.6868 - val_loss: 0.8703 - val_acc: 0.7759\n",
      "Epoch 84/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2293 - acc: 0.6865 - val_loss: 0.8762 - val_acc: 0.7741\n",
      "Epoch 85/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2265 - acc: 0.6872 - val_loss: 0.8738 - val_acc: 0.7752\n",
      "Epoch 86/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2267 - acc: 0.6870 - val_loss: 0.8744 - val_acc: 0.7764\n",
      "Epoch 87/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2267 - acc: 0.6874 - val_loss: 0.8836 - val_acc: 0.7729\n",
      "Epoch 88/100\n",
      "4187/4187 [==============================] - 283s - loss: 1.2257 - acc: 0.6870 - val_loss: 0.8686 - val_acc: 0.7763\n",
      "Epoch 89/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2249 - acc: 0.6881 - val_loss: 0.8709 - val_acc: 0.7755\n",
      "Epoch 90/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2243 - acc: 0.6878 - val_loss: 0.8766 - val_acc: 0.7756\n",
      "Epoch 91/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2235 - acc: 0.6880 - val_loss: 0.8687 - val_acc: 0.7768\n",
      "Epoch 92/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.2245 - acc: 0.6881 - val_loss: 0.8719 - val_acc: 0.7764\n",
      "Epoch 93/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2252 - acc: 0.6875 - val_loss: 0.8675 - val_acc: 0.7761\n",
      "Epoch 94/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2234 - acc: 0.6882 - val_loss: 0.8702 - val_acc: 0.7749\n",
      "Epoch 95/100\n",
      "4187/4187 [==============================] - 276s - loss: 1.2250 - acc: 0.6877 - val_loss: 0.8669 - val_acc: 0.7776\n",
      "Epoch 96/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2214 - acc: 0.6893 - val_loss: 0.8768 - val_acc: 0.7765\n",
      "Epoch 97/100\n",
      "4187/4187 [==============================] - 277s - loss: 1.2206 - acc: 0.6889 - val_loss: 0.8770 - val_acc: 0.7773\n",
      "Epoch 98/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2220 - acc: 0.6883 - val_loss: 0.8665 - val_acc: 0.7758\n",
      "Epoch 99/100\n",
      "4187/4187 [==============================] - 279s - loss: 1.2207 - acc: 0.6892 - val_loss: 0.8666 - val_acc: 0.7763\n",
      "Epoch 100/100\n",
      "4187/4187 [==============================] - 278s - loss: 1.2240 - acc: 0.6876 - val_loss: 0.8695 - val_acc: 0.7763\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                            steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                            epochs=epochs,\n",
    "                            callbacks = callbacks,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            workers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f68eac02bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
